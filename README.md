# Boxing AI Pipeline

This project builds a pose-based boxing-move classifier end-to-end: from raw video to live webcam inference. Below is an overview of each stage, directory layout, and how to reproduce or extend the pipeline.

---

## üöÄ Overview

1. **Data acquisition**
   Collect raw boxing videos (sparring, bag work, punch drills).

2. **Timestamp labeling & segmentation**
   Manually label start/end times of individual moves in an Excel sheet, then automatically slice your raw videos into fixed-length clips (MP4s) with `data_processing.ipynb` / `training.py`.

3. **Pose extraction**
   Run a YOLOv11-pose model on each clip to extract per-frame 17 keypoints ‚Üí save as NumPy arrays (`.npy`) and generate a `labels.csv`.

4. **Visualization (optional)**
   Overlay keypoints and skeleton on videos to sanity-check your pose data.

5. **Train temporal classifier**
   Use sliding windows of pose sequences to train an LSTM that maps a sequence of 50 frames ‚Üí one of eight move classes.

6. **Live inference**
   Load the trained LSTM + YOLO-pose and predict moves in real time from your webcam.

---

## üìÇ Directory Structure

```
BOXING_AI_WS/
‚îú‚îÄ‚îÄ data_raw/                      # Your uncut source videos
‚îÇ   ‚îú‚îÄ‚îÄ punching_bag_fast/
‚îÇ   ‚îú‚îÄ‚îÄ repeated_punches/
‚îÇ   ‚îî‚îÄ‚îÄ sparring_pov/
‚îÇ
‚îú‚îÄ‚îÄ time_stamps/
‚îÇ   ‚îî‚îÄ‚îÄ Boxing_Videos_Timestamp.xlsx  # Excel with manually labeled Start/End times
‚îÇ
‚îú‚îÄ‚îÄ dataset/                       # Auto-cut MP4 clips, organized by label
‚îÇ   ‚îú‚îÄ‚îÄ block/
‚îÇ   ‚îú‚îÄ‚îÄ cross/
‚îÇ   ‚îú‚îÄ‚îÄ idle/
‚îÇ   ‚îú‚îÄ‚îÄ jab/
‚îÇ   ‚îú‚îÄ‚îÄ left_hook/
‚îÇ   ‚îú‚îÄ‚îÄ left_uppercut/
‚îÇ   ‚îú‚îÄ‚îÄ right_hook/
‚îÇ   ‚îî‚îÄ‚îÄ right_uppercut/
‚îÇ
‚îú‚îÄ‚îÄ dataset_with_poses/            # NumPy pose arrays + labels.csv
‚îÇ   ‚îú‚îÄ‚îÄ block/          *.npy      # one .npy per clip, shape (T,34)
‚îÇ   ‚îú‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ labels.csv      mapping from npy_path ‚Üí label
‚îÇ
‚îú‚îÄ‚îÄ dataset_with_poses_visualization/
‚îÇ   ‚îî‚îÄ‚îÄ block/
‚îÇ       ‚îú‚îÄ‚îÄ *_annotated.mp4        # sample pose-overlaid videos (sanity check)
‚îÇ
‚îú‚îÄ‚îÄ models/                        # (optional) store custom weights or metadata
‚îÇ
‚îú‚îÄ‚îÄ best_boxing_lstm.pth           # Trained LSTM checkpoint (best on validation)
‚îÇ
‚îú‚îÄ‚îÄ data_processing.ipynb          # Jupyter notebook for cutting clips & extracting poses
‚îú‚îÄ‚îÄ training.ipynb                 # Notebook for training LSTM interactively
‚îÇ
‚îú‚îÄ‚îÄ training.py                    # Standalone training script (train_boxing_moves.py)
‚îú‚îÄ‚îÄ run_inference.py               # Real-time webcam demo script
‚îÇ
‚îî‚îÄ‚îÄ README.md                      # ‚Üê you are here
```

---

## üõ†Ô∏è Prerequisites

- **Python 3.8+**
- **FFmpeg** (for video cutting & imageio)
- **pip packages**

  ```bash
  pip install ultralytics torch torchvision \
              opencv-python-headless imageio \
              numpy pandas scikit-learn
  ```

  > **Note:** If you want GUI windows (`cv2.imshow`) in `run_inference.py`, install Ubuntu‚Äôs `python3-opencv` instead of the headless wheel.

---

## ‚ñ∂Ô∏è Step-by-Step Usage

### 1. Segment your raw videos

1. Open `time_stamps/Boxing_Videos_Timestamp.xlsx`
2. Label start/end times for each move in one video (for now).
3. Run:

   ```bash
   python training.py
   ```

   or execute cells in `data_processing.ipynb`.

4. Check `dataset/<label>/*.mp4` for your cut clips.

### 2. Extract poses

```bash
# In a Jupyter cell or script:
# configure paths at top of data_processing.ipynb:
#   POSE_MODEL_PATH, DATASET_ROOT, OUTPUT_ROOT
# then run the cell to populate dataset_with_poses/
```

- VM: `dataset_with_poses_visualization/` will contain a few annotated MP4s.

### 3. Train your LSTM model

```bash
python training.py
```

Or in Jupyter:

1. Open `training.ipynb`.
2. Run cells 1‚Äì4 to prepare data, define model, and loaders.
3. Run the **Training** cell (with sliding windows, augmentation, early stopping).
4. Your best checkpoint will be saved as `best_boxing_lstm.pth`.

### 4. Real-time inference demo

```bash
python run_inference.py
```

- Opens your laptop camera, overlaying skeleton+predicted move.
- Press **q** to quit.

**Tip:** To test on a saved video, replace the `VideoCapture(0)` line with your MP4 path.

---

## üéØ Next Steps

- **Label more data** in `time_stamps` ‚Üí bigger `dataset/`
- **Tune hyperparameters** (window length, step, hidden size, dropout)
- **Try alternative models** (Temporal CNN, Transformer)
- **Deploy** the ONNX export of your LSTM for embedded devices

---
